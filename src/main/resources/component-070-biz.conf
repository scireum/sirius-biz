product {
    modules {
        sirius-biz {
            version = "${project.version}"
            build = "${build.number}"
            date = "${timestamp}"
            vcs = "${build.vcs.number}"
        }
    }

    # Can be used to lock the system (enable the maintenance mode) right from the start of the system.
    #
    # Note that this is more of an exotic setting as it should only be used for cold- or hot-standby systems,
    # which might be brought up during a disaster scenario. The system can be used as a backup system, while the primary
    # instance recovers. The backup system should mutate as little data as possible. This is because it possibly runs on
    # (slightly) outdated data, and merging any modification back into the recovered primary system is a nightmare.
    # Therefore this setting is used to lock the system by default (which can still be disabled via the
    # admin UI /system/disaster).
    #
    # Note that once a message is specified here, the system is locked automatically on startup. If the value if left
    # empty, nothing will happen.
    defaultMaintenanceMessage = ""
}

# sirius-biz provides a multitude of frameworks for different use-cases
# and implements them using different databases. Therefore by default
# all frameworks are disabled and have to be enabled by the application.
sirius.frameworks {

    # User-manager which supports multi-tenant applications
    biz.tenants = false

    # Enables the JDBC storage layer for tenants.
    biz.tenants-jdbc = false

    # Enables the MongoDB storage layer for tenants.
    biz.tenants-mongo = false

    # A framework for letting a user map codes to text values
    biz.code-lists = false

    # Enables the JDBC storage layer for code-lists.
    biz.code-lists-jdbc = false

    # Enables the MongoDB storage layer for code-lists.
    biz.code-lists-mongo = false

    # Utilizes Elasticsearch to store all recorded logs, incidents,
    # audit logs and mails
    biz.protocols = false

    # Provides a change log for all entities which include a JournalData
    biz.journal = false

    # Provides an ID generator which can either use MongoDB or JDBC
    # to generate sequences of unique IDs
    biz.sequences = false

    # Provides distributed locks based on either the JVM, Redis or JDBC
    biz.locks = false

    # Provides a object store like API for storing files. This can either use
    # the file system or other storage facilities.
    biz.storage = false

    # Provides the metadata storage for the layer 2 by storing all metadata in a JDBC datasource
    biz.storage-blob-jdbc = false

    # Provides the metadata storage for the layer 2 by storing all metadata in MongoDB
    biz.storage-blob-mongo = false

    # Provides a replication system for stored objects by using a JDBC datasource as
    # metadata repository.
    biz.storage-replication-jdbc = false

    # Provides a replication system for stored objects by using a MongoDB as
    # metadata repository.
    biz.storage-replication-mongo = false

    # Determines if the 7-ZIP native bindings will be enabled or not.
    biz.seven-zip = false

    # Provides a rate-limiting / firewall which is either based on Redis.
    biz.isenguard = false

    # Provides a framework to record and visualize the output of background processes.
    biz.processes = false

    # Provides a framework to execute all kinds of application defined jobs.
    biz.jobs = false

    # Provides a framework store the job presets in a JDBC database.
    biz.job-presets-jdbc = false

    # Provides a framework store the job presets in a MongoDB database.
    biz.job-presets-mongo = false

    # Provides a framework to execute planned / scheduled tasks which are stored in a JDBC database.
    biz.scheduler-jdbc = false

    # Provides a framework to execute planned / scheduled tasks which are stored in a MongoDB database.
    biz.scheduler-mongo = false

    # Provides a storage option to place execution flags in a JDBC database.
    biz.analytics-execution-flags-jdbc = false

    # Provides a storage option to place execution flags in a MongoDB.
    biz.analytics-execution-flags-mongo = false

    # Provides a storage option for metrics in a JDBC database.
    biz.analytics-metrics-jdbc = false

    # Provides a storage option for in a MongoDB.
    biz.analytics-metrics-mongo = false

    # Uses MongoDB to store and manage tenant specific scripts to handle incoming
    # script event (e.g. in import processes).
    biz.scripting-mongo = false

    # Provides an uplink to talk to one or more Jupiter instances.
    jupiter = false

    # Provides a searchable and simple knowledge base built by simple pasta files in the resource directory "kb".
    tycho.knowledge-base = false

    # Provides a system-wide search.
    tycho.open-search = false

    # Enables JDBC/SQL based onboarding video academies.
    tycho.academies-jdbc = false

    # Enables MongoDB based onboarding video academies.
    tycho.academies-mongo = false

}

scripting {
    sandbox {
        rules {
            "sirius.kernel.commons.Context.*" = true
            "sirius.biz.process.ErrorContext.*" = true
            "sirius.db.mixing.BaseEntity.getId" = true
            "sirius.db.mixing.BaseEntity.getIdAsString" = true
            "sirius.db.mixing.BaseEntity.getUniqueName" = true
            "sirius.db.mixing.BaseEntity.isNew" = true
            "sirius.db.mixing.types.BaseEntityRef.fetchValue" = true
            "sirius.db.mixing.types.BaseEntityRef.fetchCachedValue" = true
            "sirius.db.mixing.types.BaseEntityRef.getId" = true
            "sirius.db.mixing.types.BaseEntityRef.getIdAsString" = true
            "sirius.db.mixing.types.BaseEntityRef.is" = true
            "sirius.db.mixing.types.BaseEntityRef.isFilled" = true
            "sirius.db.mixing.types.SafeList.contains" = true
            "sirius.db.mixing.types.SafeList.isEmpty" = true
            "sirius.db.mixing.types.SafeList.isFilled" = true
        }
    }
}

# Add our SCSS to tycho.scss from sirius-web...
assets.scss.tycho {
    biz {
        path = "/assets/styles/tycho-biz.scss"
    }
}

# Place the local address of the node here, i.e. http://192.168.0.1
# If no address is given, we use the local address determined by the system
# (using: InetAddress.getLocalHost().getHostAddress()) - however, in some
# environments like Docker, this might yield an inappropriate address.
#
# This address has to be reachable from all other cluster nodes.
sirius.nodeAddress = ""

# Contains a local token which is used by the Cluster controller so that some APIs can be invoked without
# further authentication.
sirius.clusterToken = ""

# By default the fallback language for the knowledge base is english.
knowledgebase.fallbackLanguages = ["en"]

# Contains settings for the built-in firewall and rate-limiting facility
isenguard {
    # Determines which limiter is used. By default we use a "smart" strategy,
    # which uses "redis" is available and otherwise switches to the "noop" limiter.
    limiter = "smart"

    # If the "Redis" limiter is used, the given redis database is used to store
    # the counters and blocked IPs. By default we use the "system" database,
    # which is the default redis.
    redisName = "system"

    # Contains an interval and limit per interval for each realm.
    # Note that the realm "http" is used to limit all notable
    # HTTP calls.
    limit {
        # Specifies the defaults for all realms unless noted otherwise.
        # By default IsenGuard is turned off.
        default {
            # Defines the check interval
            interval = 10m

            # Defines the max number of occurrences within the given inverval
            limit = 0

            # Declares which the "scope" value of this realm will be.
            # There are three standard types:
            # - ip:     Limiting by ip address
            # - tenant: Limiting by tenant id
            # - user:   Limiting by user id
            #
            # For all other kinds of realms, the type "custom" can be used.
            # Also, additional frameworks might define more types.
            type = "custom"
        }

        # Specifies the constraints for all HTTP requests. By default this
        # is turned off, as there is no way of knowing the usage pattern
        # of a specific application.
        http {
            interval = 0
            limit = 0
            type = "ip"
        }

        # Specifies the constraints for negative AuditLog events (wrong password etc).
        # Once this limit is hit, the calling IP will be blocked for ten minutes.
        security {
            interval = 2m
            limit = 50
            type = "ip"
        }
    }

}

http.api {
    # Lists job and process apis as provided by the platform.
    jobs-processes {
        priority = 800
        label = "Jobs and Processes"
        description = "Provides services to start jobs and to monitor process execution."
        requiredRoles = "permission-view-processes"
    }

    # Lists the cluster management services as provided by the platform.
    cluster {
        priority = 810
        label = "Cluster Health and Maintenance"
        description = "Provides services to monitor and control the cluster and node state."
        requiredRoles = "flag-system-tenant"
        documentationUri = "/kba/V7FGJ"
    }

    # Provides a documentation URI to the API otherwise defined by sirius-web.
    health {
        documentationUri = "/kba/5ZM4U"
    }
}

health.limits {
    # If there is any lock held, we will report this - but there is no
    # sane limit how many locks can be considered healthy / unhealty
    locks-count.gray = 1
    locks-count.warning = 0
    locks-count.error = 0

    # We start to warn as soon as we encounter one long running lock
    # (held for at least 30min). As this can still be quite alright
    # we do not consider this critical (red)
    locks-long-running.gray = 0
    locks-long-running.warning = 1
    locks-long-running.error = 0

    # Monitors the utilization of the events buffer which is used by the
    # EventRecorder to permit batch inserts of recorded events into Clickhouse
    events-buffer-usage.gray = 0
    events-buffer-usage.warning = 80
    events-buffer-usage.error = 99

    # Number of active tasks (remains gray when zero). There is no limit
    # to warn about, as the number can be specified in the system configuration.
    active-distributed-tasks.gray = 1
    active-distributed-tasks.warning = 0
    active-distributed-tasks.error = 0

    # Reports the number of uploads into the storage system. There are no
    # real upper limits to enforce.
    storage-uploads.gray = 1
    storage-uploads.warning = 0
    storage-uploads.error = 0

    # Reports the number of downloads from the storage system. There are no
    # real upper limits to enforce.
    storage-downloads.gray = 1
    storage-downloads.warning = 0
    storage-downloads.error = 0

    # Reports the number of fetches (as stream) from the storage system. There are no
    # real upper limits to enforce.
    storage-streams.gray = 1
    storage-streams.warning = 0
    storage-streams.error = 0

    # Reports the number of deliveries (via HTTP) from the storage system. There are no
    # real upper limits to enforce.
    storage-deliveries.gray = 1
    storage-deliveries.warning = 0
    storage-deliveries.error = 0

    # Reports a warning as soon as we start to deliver data from the fallback location.
    storage-fallbacks.gray = 0
    storage-fallbacks.warning = 1
    storage-fallbacks.error = 0

    # Reports if there are any client errors like "404 - file not found".
    storage-client-errors.gray = 1
    storage-client-errors.warning = 0
    storage-client-errors.error = 0

    # Warns if there are any server errors which couldn't be healed by serving from
    # the fallback location.
    storage-server-errors.gray = 0
    storage-server-errors.warning = 1
    storage-server-errors.error = 0

    # Reports the number of replication tasks executed by this node.
    storage-replication-tasks.gray = 1
    storage-replication-tasks.warning = 0
    storage-replication-tasks.error = 0

    # Reports the duration of replication tasks executed by this node.
    storage-replication-duration.gray = 1
    storage-replication-duration.warning = 0
    storage-replication-duration.error = 0

    # Reports the number of conversions executed by this node.
    storage-conversions.gray = 1
    storage-conversions.warning = 0
    storage-conversions.error = 0

    # Reports the duration of conversions executed by this node.
    storage-conversion-duration.gray = 1
    storage-conversion-duration.warning = 0
    storage-conversion-duration.error = 0

    # Reports the number of Jupiter calls.
    jupiter-calls.gray = 1
    jupiter-calls.warning = 0
    jupiter-calls.error = 0

    # Reports the average call duration of Jupiter (in millis)
    jupiter-call-duration.gray = 1
    jupiter-call-duration.warning = 1000
    jupiter-call-duration.error = 0

}

# Specifies thread pools used by the biz platform
async {
    executor {
    # Defines the maximal number of concurrent tasks executed for the
    # Distributed Tasks framework.
        distributed-tasks {
            poolSize = 8

            # Having a queue would be pointless, as the WorkLoaderLoop only
            # tries to keep the available executors running but will not
            # schedule additional work.
            queueLength = 0
        }

        # Interactive jobs should actually execute quite instantly. Therefore
        # we only permit a low parallelism but a certain queue length for peak loads.
        interactive-jobs {
            poolSize = 2
            queueLength = 100
        }

        # Used to handle all OpenSearch requests in Tycho. As this is quite a database intense
        # task, we pick quite a small number here but provide a queue to provide some backlog.
        # When worse comes to worst, we handle a failure gracefully and show the user an overload
        # hint.
        tycho-open-search {
            poolSize = 2
            queueLength = 32
        }

        # Executes the search tasks per provider. As some providers are faster than others, we provide a
        # bit more parallelism here. If this thread pool is exhausted, the tasks will be handled in the
        # tycho-open-search pool directly so that a proper back pressure is applied.
        tycho-open-search-task {
            poolSize = 4
            queueLength = 32
        }

        # Limits the maximal number of parallel threads occupied by the DataExplorer, as computing charts
        # might be time-consuming/slow...
        data-explorer {
            poolSize = 8
            queueLength = 512
        }

        # This work queue is shared by all transfer managers across all object stores and
        # used for multipart up- and downloads.
        s3 {
            poolSize = 10
            queueLength = 0
        }

        # Provides a smaller "work stealing" pool which is used by Processes (ProcessContext) to execute
        # small side tasks in parallel. Note that we explicitly do not want to have a queue here as
        # we'd rather block the "main thread" than queue up tasks.
        process-sidetask {
            poolSize = 16
            queueLength = -1
        }

        # This executor collects all requests which are served via the storage framework. These requests might
        # be blocked as we might need to wait for a conversion to finish.
        storage-conversion-delivery {
            poolSize = 16
            queueLength = 128
        }

        # This executor is used by the storage framework (layer 2) to actually perform the conversion / generation of
        # a variant of a blob.
        storage-conversion {
            poolSize = 8
            queueLength = 1024
        }

        # Large files are tunneled via a blocking approach within the Layer 1 of the storage framework. Most notably,
        # this is enabled by the BlobDispatcher for file known to be large. We use this approach, to safely shovel
        # multi-gigabyte files without the risk of a consuming all IO buffers for a single download due to contention.
        storage-deliver-large-file {
            poolSize = 128
            queueLength = 512
        }

        # Complex deletes should actually execute quite instantly. Therefore
        # we only permit a low parallelism but a certain queue length for peak loads.
        complex-deletes {
            poolSize = 2
            queueLength = 200
        }
    }

    distributed {
        # Configures the nature of the queues used to distribute tasks.
        queues {
            # Each queue needs to suppliy the following settings
            # example {
                # Contains the concurrency token to control node-local parallelism
                # concurrencyToken = SomeToken

                # Determines if the queue is prioritized or a FIFO queue
                # prioritized = false

                # For prioritized queues the penalty should approximately be
                # equal to the expected runtime of an average task. This time is
                # used to compute the effective execution priority once a task is
                # scheduled.
                # penaltyTime = 1 minute
            # }

            # Defines the queue used by the MetricsGuaranteedSchedulerExecutor when scheduling metrics.
            metrics-scheduler {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the MetricsGuaranteedBatchExecutor when executing batches of metrics tasks.
            metrics-batch {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the MetricsBestEffortSchedulerExecutor and MetricsBestEffortBatchExecutor
            # when scheduling and executing batches of "best effort" metrics tasks.
            #
            # These tasks are only scheduled if the queue is empty. Therefore, if the system is overloaded it may
            # skip some of the "best effort" tasks - but it will never skip a guaranteed task.
            metrics-best-effort {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the CheckBatchExecutor to execute DailyChecks and ChangeChecks.
            checks {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the DefaultBatchProcessTaskExecutor for miscellaneous jobs.
            jobs {
                # We use a simple token here so that the number of parallel jobs can be specified
                concurrencyToken = "small-jobs"

                # The queue will be prioritized
                prioritized = true

                # As we have no idea what the average runtime of a job in this queue might be, we default the penalty
                # time to one hour.
                penaltyTime = 1h
            }

            # Defines the queue used by the ImportBatchProcessTaskExecutor for import jobs.
            import-jobs {
                concurrencyToken = "large-jobs"
                prioritized = true
                penaltyTime = 2h
            }

            # Defines the queue used by the ExportBatchProcessTaskExecutor for export jobs.
            export-jobs {
                concurrencyToken = "large-jobs"
                prioritized = true
                penaltyTime = 30m
            }

            # Defines the queue used by the ReportBatchProcessTaskExecutor for report jobs.
            report-jobs {
                concurrencyToken = "small-jobs"
                prioritized = true
                penaltyTime = 30m
            }

            # Defines the queue used by the CheckBatchProcessTaskExecutor for check jobs.
            check-jobs {
                concurrencyToken = "small-jobs"
                prioritized = true
                penaltyTime = 1h
            }

            # Provides the config for the queue which is used by the storage layer 1 to perform
            # replication tasks.
            storage-layer1-replication {
                concurrencyToken = "replication"
                prioritized = false
            }
        }

        # Configures concurrency tokens which are semaphores on each node and
        # control local parallelism. Note that a single token can be shared by
        # multiple queues.
        concurrency {
            # Specifies the maximal number of parallel small jobs.
            small-jobs = 4

            # Specifies the maximal number of parallel large jobs.
            large-jobs = 2

            # Specifies the maximal number of parallel analytical tasks.
            analytics = 2

            # Specifies the maximal number of parallel replication tasks to perform.
            replication = 2
        }
    }
}

# Provides a cluster wide controller for executing background jobs.
# NeighborhoodWatch uses Redis locks and timestamps to control the
# execution of background jobs across a cluster of nodes.
# Per job one for the following settings can be set:
# LOCAL    - the jobs runs on this node independently of the cluster
# CLUSTER  - the job may run on this node, but only on one node within the cluster at once
# DISABLED - the job is disabled on this node
orchestration {
    loop-elastic-auto-batch = LOCAL
    loop-event-processor = LOCAL
    loop-delay-line = LOCAL
    loop-distributed-tasks-work-loader = LOCAL
    loop-redis-limiter-cleanup = CLUSTER
    loop-job-scheduler = CLUSTER
    loop-storage-layer1-replication = CLUSTER
    loop-storage-layer2-process = CLUSTER
    loop-storage-layer2-touch-writeback = LOCAL
    task-analytical-engine = CLUSTER
    task-end-of-day = CLUSTER

    # TODO remove once stroage is migrated...
    task-storage-cleaner = CLUSTER
}



timer.daily {

    # Determines when protocols and journals are purged based on the given settings...
    protocols-cleaner = 2

    # Determines when old execution flags are purged...
    delete-execution-flags = 4

    # TODO kill when storage has been migrated:
    storage-cleaner = 3

    # Determines when outdated files in the blob storage system are purged...
    storage-layer2-cleaner = 3

    # Determines when analytical tasks are scheduled...
    analytical-engine = 23

    # Determines when expires process logs are removed...
    cleanup-processes = 2
}

# Controls the storage duration of protocol entries
protocols {
    keep-logs = 30 days
    keep-incidents = 30 days
    keep-mails = 365 days
    keep-journal = 1000 days
    keep-neutral-audit-logs = 30 days
    keep-negative-audit-logs = 180 days

    # We limit messages to this length when storing them in ES as otherwise we might run into
    # low memory conditions or overload the AutoBatchLoop and ES. Note that the logging system which
    # captures stdout will still receive the full message. Still consider to limit yourself to
    # sane and digestible log messages.
    maxLogMessageLength = 116384
}

# The audit log can write additional logs to the system log.
# This can be used to preserve the audit logs in an external archive with a
# potentially longer storage period. By default this is disabled to not jam
# the system logs.
logging.audit = OFF

# Specifies the secret used to sign internal URLs for specific entities...
# An empty secret signals, that a new (local) secret es generated during startup...
controller.secret = ""

# Defines default code lists known to the system.
code-lists {
    default {
        # Determines if unknown codes should be recorded automatically so that
        # they can be managed in the admin UI
        autofill = true

        # Determines if a single code list is shared accross all tenants. These code lists
        # are owned by the system tenant and should have "autofill" turned off for obvious
        # reasons.
        global = false

    }

    # Defines the list of salutations.
    salutations {
        name = "Salutations"
        description = "Contains all salutations known to the system"
    }

    # Defines the list of countries.
    countries {
        name = "Countries"
        description = "Contains all countries known to the system. A RegEx can be supplied as additional value which is used to verify ZIP codes"
    }

    # Defines the list of languages.
    languages {
        autofill = false
        global = true
        name = "Languages"
        description = "Contains all languages known to the system."
    }
}

# Provides the settings which determine how LookupTables are fueled with data.
#
# A LookupTable can either access a CodeList and use the data provided there. Or it can access Jupiter / InfoGraphDB
# tables and use this data. Furthermore such Jupiter tables can be combined, so that a default table (e.g. defined
# by the product) and a custom table (e.g. defined by the local installation) can be combined.
#
# Additionally a Jupiter table can be filtered by a IDBSet (e.g. when using a table which defines all known ISO
# countries but the actual lookup table should only include countries which are of interest by a particular use case).
# Using this approach, the same table can be used in different lookup tables with either distinct or overlapping sets
# of codes being defined.
#
# If no config is found for a lookup table, we use the default config as provided below and access the code list with
# the same name.
lookup-tables {
    default {
        # Determines if the table supports scanning. Scanning in this case means listing all possible codes (most
        # probably when recommending possible codes in the UI). If this is set to false, we only provide a search view
        # which is more appropriate for large lists (e.g. > 100ish items).
        supportsScan = true

        # Determines how the "code" field is normalized. Using "lower", all values are forced to be lowercase. When
        # "upper" is used, all values are forced to be uppercase. If left empty, the codes are taken as given by the
        # user (which is sort of error prone and thus not recommended).
        codeCase = "upper"

        # Contains the name of the IDB table field which contains the main/normalized code
        codeField = "code"

        # Contains the name of the IDB table field which contains the name
        nameField = "name"

        # Contains the name of the IDB table field which contains a short description of an entry
        descriptionField = "description"

        # Contains the list of additional IDB table fields which may contain alternative codes when normalizing
        # user input.
        aliasCodeFields = [ "aliases" ]

        # Contains the name of the dictionary which contains mappings to other codes of different standards etc.
        mappingsField = "mappings"
    }

    # Specifies the lookup table which is used by "Countries".
    # Note that this can be further customized by applications (i.e. store in Jupiter etc.)
    countries {
        codeCase = "lower"
    }

    # Specifies the lookup table which lists all actively enabled countries in "Countries".
    active-countries {
        # Note that by default we simply redirect this to the "countries" code list. Therefore all countries
        # and active countries are the same. Once an application specifies a "table" and or a "filterSet",
        # this will win over the code list...
        codeList = "countries"
        codeCase = "lower"
        title = "$LookupTable.countries"
    }

    # Specifies the lookup table which is used by "Languages".
    # Note that this can be further customized by applications (i.e. store in Jupiter etc.)
    languages {
        codeCase = "lower"
    }

    # Specifies the lookup table which lists all actively enabled languages in "Languages".
    active-languages {
        # Note that by default we simply redirect this to the "languages" code list. Therefore all languages
        # and active languages are the same. Once an application specifies a "table" and or a "filterSet",
        # this will win over the code list...
        codeList = "languages"
        codeCase = "lower"
        title = "$LookupTable.languages"
    }

    # Specifies the lookup table which is used by "Salutations".
    # Note that this can be further customized by applications (i.e. store in Jupiter etc.)
    salutations {
        codeList = "salutations"
    }

    # Provides an example of a fully populated lookup table entry.
    # example {
    #    # Provides the name of the code list to use (in case this differs from the id of this table)
    #    codeList = ""
    #
    #    # Contains the name of the Jupiter IDB table to use. This value wins over "codeList".
    #    table = ""
    #
    #    # May contain the name of the additional IDB table which is combined with the main table to support
    #    # customizations.
    #    customTable = ""
    #
    #    # May contain the name of an IDB set which actually filters which values in the table are accepted by the
    #    # lookup table.
    #    filterSet = ""
    # }
}

# Contains the configuration of the legacy file / object store system.
storage {

    # In order to support access control over multiple nodes, a shared secret must be placed here
    # (in the instance.conf) so that each node can verify any links / downloads geneared by other
    # nodes. If this remains empty, each node will generate its own internal secret.
    # sharedSecret = "VERY LONG AND SECURE SECRET"

    # Defines the base directory when storing buckets in disk.
    baseDir = "data/storage"

    # If using ImageMagick, consider a command like:
    # "convert ${src} -resize ${width}x${height}> -quiet -quality 89 -format ${imageFormat} -strip -colorspace RGB -background white ${extend} -flatten ${dest}"
    conversionCommand = ""

    # Option for the conversion command to extend the image to a minimum size
    extendOption = "-gravity center -extent ${extendWidth}x${extendHeight}<"

    # Defines all buckets known to the system.
    buckets {
        default {
            # Defines the permission required to view the bucket in the management UI.
            permission = "permission-manage-files"

            # Determines if an object (file) can be created via the management UI.
            canCreate = false

            # Determines if an object (file) can be edited via the management UI.
            canEdit = false

            # Determines whether a search in a bucket should always use a like constraint.
            alwaysUseLikeSearch = false

            # Determines if an object (file) can be deleted via the management UI.
            canDelete = false

            # Determines if objects are automatically removed after N days. 0 means disabled.
            deleteFilesAfterDays = 0

            # Determines the storage engine used for the bucket.
            engine = "fs"

            # Whether uses of the bucket should be logged into the deprecated log if it's log level is FINE.
            logAsDeprecated = false
        }

        # A work directory / bucket is provided per tenant and can be used to in- and output files.
        # This is also visible in the built-in virtual file system (FTP server) to upload and download files.
        # To limit the number of files in this directory, old files (older than 30 days) are automatically removed.
        # Therefore this should not be used for permanent storage.
        work {
            canCreate = true
            canEdit = true
            canDelete = true
            deleteFilesAfterDays = 30
        }

        # Provides a temporary storage space which is automatically maintained (files are deleted after 30 days).
        tmp {
            permission = "permission-manage-admin-files"
            canCreate = true
            canEdit = true
            canDelete = true
            deleteFilesAfterDays = 30
        }

        # Defines storage for versioned files
        versioned-files {
            canCreate = false
            canEdit = false
            canDelete = false

            # number of versions kept from one versioned file
            # setting this number to 0 will keep all versions
            maxNumberOfVersions = 50
        }
    }

}

# Provides credentials for the S3 compatible stores managed by ObjectStores.
s3 {
    stores {
        # Provides the default configuration shared by all stores.
        default {
            accessKey = ""
            secretKey = ""
            endPoint = ""
            bucketSuffix = ""
            pathStyleAccess = true
            # Defines the time to wait for a transfer to finish once it has been started. We had it previously tuned up to 5 minutes.
            # the default value defined in com.amazonaws.ClientConfiguration is 50s
            socketTimeout = 5m
            # Defines the time to wait for a connection to be established. 10s is the default value defined in com.amazonaws.ClientConfiguration
            connectionTimeout = 10s
            # Defines the size of the connection pool. 50 is the default value defined in com.amazonaws.ClientConfiguration
            maxConnections = 50
            # Defines how long a connection can be idle before it is closed. If set to a value <= 0, connections are never closed
            # which is the default value defined in com.amazonaws.ClientConfiguration
            connectionTTL = 0s

            # Specifies the signer to use. Leave empty to use the standard signer of the
            # current AWS SDK.
            signer = ""
            # Use the following setting for CEPH stores:
            # signer = "S3SignerType"
        }

        # By default a "system" store is used if no other name is given.
        # An application should provide a configuration for this store if ObjectStores are used.
        system {

        }
    }
}

# Contains settings for the virtual file system.
storage {

    # Contains settings for the physical storage layer which either persists objects onto
    # disk or into a S3 compatible store.
    layer1 {

        # Defines the replication settings
        replication {
            # Defines the number of replication tasks in a batch.
            batchSize = 100

            # Defines the minimal number of batches in the queue. The ReplicationBackroundLoop will only push more
            # batches into the queue if less than this number are already queued.
            minBatches = 250

            # Defines the max number of batches to queue per execution.
            maxBatches = 250

            # Defines the duration for which a delete is delayed.
            replicateDeleteDelay = 45d

            # Defines the duration for which an update is delayed.
            replicateUpdateDelay = 5m

            # Contains the duration for which a retry of a failed replication task is delayed.
            retryReplicationDelay = 90m

            # Determines the max number of replication attempts before a task is considered as "failed".
            maxReplicationAttempts = 5
        }

        # Enumerates the physical storage spaces known to the system.
        spaces {
            # Defines the defaults shared by all spaces.
            default {
                # Defines the engine to use. This can be either "s3" or "fs".
                engine = "s3"

                # Defines the s3 store to use (defined in s3.stores)
                store = "system"

                # Defines the bucket to use. If left empty, the space name will be used along with
                # the suffix as defined in s3.stores for the selected store.
                bucketName = ""

                # For the "fs" engine a baseDir can be defined where a spaces are placed unless configured
                # otherwise (see below).
                baseDir = "data/storage"

                # Defines an effective path to store object into when using the "fs" engine.
                # If left empty, the "baseDir" + space name is used as path.
                path = ""

                # Defines the compression level to apply when storing objects.
                # Possible values are: off, fast, default, best (see CompressionLevel)
                compression = "off"

                # Selects the cipher factory to use for encrypting and decrypting data.
                # No encryption will be used when left empty.
                # Possible values are any name of a CipherFactory e.g. "aes256" (AES256CipherFactory)
                cipher = ""

                # Specifies the passphrase used to initialize the cipher provider
                # (e.g. used by the AES256CipherFactory).
                passphrase = ""

                # Can be used to configure the backup storage space for this space. This has to be the
                # name of another configured layer1 storage space. The replication manager will then
                # pick this up and transfer all objects to the backup.
                replicationSpace = ""
            }

            # Defines the layer 1 settings for the work space
            work {
                # This is empty as we stick with the default settings.
            }

            # Defines the layer 1 settings for the temporary storage space
            tmp {
                # This is empty as we stick with the default settings.
            }

            # Defines the layer 1 settings of the storage space used for process files.
            processes {
                # This is empty as we stick with the default settings.
            }

            # Defines the layer 1 settings of the storage space used for tenant images.
            tenants {
                # This is empty as we stick with the default settings.
            }

            # Defines the layer 1 settings of the storage space used for user account images.
            user-accounts {
                # This is empty as we stick with the default settings.
            }
        }
    }

    # Contains settings for the blob storage system.
    layer2 {

        # Determines the limit when the URLBuilder switches to /dasd/xxl/... URLs. These can be detected by
        # upstream reverse proxies and used to detect large files which aren't worth or healthy to cache.
        # Note that this check has to be enabled manually using URLBuilder.enableLargeFileDetection.
        largeFileLimit = 128M

        # Controls the conversion settings used by the BlobStorageSpace to generate variants of a blob.
        conversion {

            # Determines if conversions are enabled on this node
            enabled = true

            # Contains a list of hostnames or IP addresses to which a conversion can be delegated by
            # forwarding a request.
            hosts = []

            # Specifies the total number of attempts for the optimistic locking for conversion.
            # This basically controls parallel requests to database operations (creating or updating blobs and variants).
            maxOptimisticLockAttempts = 5

            # Specifies the total number of attempts to wait for a conversion result.
            maxConversionAttempts = 4

            # Specifies the total number of attempts to wait for a conversion result when requested to wait longer.
            maxLongConversionAttempts = 12

            # Specifies the number of milliseconds to wait between conversion attempts (maxConversionAttempts)
            conversionRetryDelay = 500ms

            # Specifies the interval after which likely hanging conversion is retried for a given blob variant.
            hangingConversionRetryInterval = 45m

            variants {
                # Provides a simple variant which uses the identity converter. This is mostly used as a test
                # of the conversion framework as it doesn't provide any actual benefit.
                identity {
                    # Names the converter (see config block below) to use.
                    converter = "identity"
                }

                tenant-small {
                    converter = "identity"
                }
                tenant-medium {
                    converter = "identity"
                }
                tenant-large {
                    converter = "identity"
                }
                user-small {
                    converter = "identity"
                }
                user-medium {
                    converter = "identity"
                }
                user-large {
                    converter = "identity"
                }
            }

            converters {
                # The identity converter doesn't required any configuration at all.
                identity {
                    # Contains the converter (actually the name of the ConverterFactory) to use.
                    type = "identity"
                }
            }
        }

        spaces {
            default {
                # Defines the permission which is required to browse and read the space in the layer 3. Note that
                # this is not enforced by direct (layer 2) access.
                readPermission = "enabled"

                # Defines the permission which is required to write / modify blobs in the space via the layer 3.
                # Note that this is not enforced by direct (layer 2) access.
                writePermission = "enabled"

                # Contains a custom base url (hostname) like "https://my-custom-host.io" which will be
                # used when creating delivery URLs for this space.
                baseUrl = ""

                # Determines if the file system in this space is case sensitive (false) or case insensitive
                # (true). By default, we use a case sensitive file system as known from the UNIX world.
                useNormalizedNames = false

                # Determines the retention period in days.
                # If non-zero all blobs in this space will be deleted after the given period
                # (starting from their last modified date or last touched date).
                retentionDays = 0

                # Determines the url validity time in days.
                urlValidityDays = 2

                # Determines if touch tracking (keeping the lastTouched timestamp up to date) is enabled or not.
                # If a space is heavily used, it might be necessary to turn this off (especially of the generated
                # data isn't used).
                touchTracking = true

                # Determines if files are sorted by their name ascending (false) or by their last modification
                # timestamp descending. Note that directories will always be sorted by name ascending.
                sortByLastModified = false

                # Contains a description to be shown in /fs
                description = ""

                # Contains the academy track to link to, when showing this space in /fs
                academyVideoTrackId = ""

                # Contains the academy video to link to, when showing this space in /fs
                academyVideoCode = ""

                # Contains the knowledge base article (KBA) to link to, when showing this space in /fs
                kba = ""
            }

            # Defines a work space which is shared by all users of a tenant and can be used to provide
            # an input and output to jobs etc.
            work {
                description = "$BlobStorageSpace.work.description"
                retentionDays = 30
                sortByLastModified = true
                useNormalizedNames = true
            }

            # Provides a temporary storage location which will be cleaned up every once in a while.
            # This is normally invisible and can only be used to resolve temporary files via the TmpRoot.
            tmp {
                readPermission = "disabled"
                writePermission = "disabled"
                retentionDays = 10
            }

            # This space is used to store files which are attached to Processes
            processes {
                readPermission = "disabled"
                touchTracking = false
            }

            # This space is used to store image files for Tenants
            tenants {
                readPermission = "disabled"
                touchTracking = true
                urlValidityDays = 30
            }

            # This space is used to store image files for UserAccounts
            user-accounts {
                readPermission = "disabled"
                touchTracking = true
                urlValidityDays = 30
            }
        }
    }

    layer3 {

        # Defines uplinks available to all tenants.
        roots {
            # Defines a file system uplink which makes part of the local file system visible to the VFS
            # fs {
            #    # Contains the name of the virtual directory
            #    name = "fs"
            #
            #    # Contains an optional short description of the mount point
            #    description = ""
            #
            #    # Defines the type of the uplink
            #    type = "fs"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Contains the base path to mount
            #    basePath = "/data/somewhere"
            # }

            # Defines a file system uplink which mounts a CIFS share into the VFS
            # cifs {
            #     # Contains the name of the virtual directory
            #    name = "MyShare"
            #    # Contains an optional short description of the mount point
            #    description = "Some words of caution"
            #
            #    # Defines the type of the uplink
            #    type = "cifs"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Contains the smb url to the share
            #    url = "smb://my.samba.share/path/"
            #
            #    # Contains the domain to which the user belongs which is used to authentication
            #    domain = "my-domain.local"
            #
            #    # Contains the user used to mount the share
            #    user = ""
            #
            #    # Contains the password of the user
            #    password = ""
            # }

            # Defines a file system uplink which mounts a remote SFTP server into the VFS
            # sftp {
            #     # Contains the name of the virtual directory
            #    name = "MyShare"
            #
            #    # Contains an optional short description of the mount point
            #    description = "Some words of caution"
            #
            #    # Defines the type of the uplink
            #    type = "sftp"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Determines the host to connect to
            #    host = "hostname"
            #
            #    # Determines the port to use (Default is 22)
            #    port = 22
            #
            #    # Determines the uername used to login
            #    user = "username"
            #
            #    # Determines the password used to authenticate
            #    password = "secretPassword"
            #
            #    # Contains the base path to mount (Default is "/")
            #    basePath = "/data/somewhere"
            #
            #    # Specifies the maximal number of idle connections kept in the pool (Default is 1)
            #    # maxIdle = 1
            #
            #    # Specifies the maximal number of active connections (Default is 5)
            #    # maxActive = 5
            #
            #    # Specifies the connect timeout in milliseconds (Default is 10000)
            #    # connectTimeoutMillis = 10000
            #
            #    # Specifies the read / IO timeout in milliseconds (Default is 615000 = 10min 15s)
            #    # Note that this should be higher than the idle timeout as otherwise the implementation gets royally
            #    # confused...
            #    # readTimeoutMillis = 615000
            #
            #    # Specifies the idle timeout in milliseconds. This is the maximal duration for with an unused
            #    # connection is kept open. (Default is 600_000 = 600s = 10 min)
            #    # idleTimeoutMillis = 600000
            #
            #    # Specifies the wait time in milliseconds. If the connection pool is exhaused, the is the maximal
            #    # duration we wait for a connection to become available. (Default is 10000 = 10s)
            #    # maxWaitMillis = 10000
            #
            # }
            #
            # Defines a file system uplink which mounts a remote FTP server into the VFS
            # ftp {
            #     # Contains the name of the virtual directory
            #    name = "MyShare"
            #    # Contains an optional short description of the mount point
            #    description = "Some words of caution"
            #
            #    # Defines the type of the uplink
            #    type = "ftp"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Determines the host to connect to
            #    host = "hostname"
            #
            #    # Determines the port to use (Default is 21)
            #    port = 21
            #
            #    # Determines the uername used to login
            #    user = "username"
            #
            #    # Determines the password used to authenticate
            #    password = "secretPassword"
            #
            #    # Contains the base path to mount (Default is "/")
            #    basePath = "/data/somewhere"
            #
            #    # Determines the encoding to use for the control connection (Default is UTF-8)
            #    encoding = "UTF-8"
            #
            #    # Specifies the maximal number of idle connections kept in the pool (Default is 1)
            #    # maxIdle = 1
            #
            #    # Specifies the maximal number of active connections (Default is 5)
            #    # maxActive = 5
            #
            #    # Specifies the connect timeout in milliseconds (Default is 10000)
            #    # connectTimeoutMillis = 10000
            #
            #    # Specifies the read / IO timeout in milliseconds (Default is 615000 = 10min 15s)
            #    # Note that this should be higher than the idle timeout esp. for SFTP as otherwise
            #    # the implementation gets royally confused...
            #    # readTimeoutMillis = 615000
            #
            #    # Specifies the idle timeout in milliseconds. This is the maximal duration for with an unused
            #    # connection is kept open. (Default is 600_000 = 600s = 10 min)
            #    # idleTimeoutMillis = 600000
            #
            #    # Specifies the wait time in milliseconds. If the connection pool is exhaused, the is the maximal
            #    # duration we wait for a connection to become available. (Default is 10000 = 10s)
            #    # maxWaitMillis = 10000
            #
            # }

            # Defines a file system uplink which mounts a remote FTP server with TLS/SSL into the VFS
            # ftps {
            #    # Contains the name of the virtual directory
            #    name = "MyShare"
            #    # Contains an optional short description of the mount point
            #    description = "Some words of caution"
            #
            #    # Defines the type of the uplink
            #    type = "ftps"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Determines the host to connect to
            #    host = "hostname"
            #
            #    # Determines the port to use (Default is 21)
            #    port = 21
            #
            #    # Determines the uername used to login
            #    user = "username"
            #
            #    # Determines the password used to authenticate
            #    password = "secretPassword"
            #
            #    # Contains the base path to mount (Default is "/")
            #    basePath = "/data/somewhere"
            #
            #    # Determines the encoding to use for the control connection (Default is UTF-8)
            #    encoding = "UTF-8"
            #
            #    # Specifies the maximal number of idle connections kept in the pool (Default is 1)
            #    # maxIdle = 1
            #
            #    # Specifies the maximal number of active connections (Default is 5)
            #    # maxActive = 5
            #
            #    # Specifies the connect timeout in milliseconds (Default is 10000)
            #    # connectTimeoutMillis = 10000
            #
            #    # Specifies the read / IO timeout in milliseconds (Default is 615000 = 10min 15s)
            #    # Note that this should be higher than the idle timeout esp. for SFTP as otherwise
            #    # the implementation gets royally confused...
            #    # readTimeoutMillis = 615000
            #
            #    # Specifies the idle timeout in milliseconds. This is the maximal duration for with an unused
            #    # connection is kept open. (Default is 600_000 = 600s = 10 min)
            #    # idleTimeoutMillis = 600000
            #
            #    # Specifies the wait time in milliseconds. If the connection pool is exhaused, the is the maximal
            #    # duration we wait for a connection to become available. (Default is 10000 = 10s)
            #    # maxWaitMillis = 10000
            #
            #    # Specifies the SSL protocol to use. By default, we negotiate the most current supported version
            #    # with the server. See https://docs.oracle.com/en/java/javase/24/docs/specs/security/standard-names.html#sslcontext-algorithms
            #    # for supported values.
            #    # sslProtocol = "TLSv1.2"
            # }
        }

        downlink {
            # Provides the configuration of the built-in SSH (SCP/SFTP) server.
            ssh {
                # Specifies the port to listen on. Use 0 to disable the server or 22 to run it on the common SSH port.
                port = 0

                # Contains the path to the serialized host key. Note that an external file should be used which
                # is persisted across restarts and patches as otherwise the SSH connection looses its trust.
                hostKeyFile = "hostkey.ser"

                # Specifies the idle timeout for connections.
                idleTimeout = 30m

                # Specifies the read timeout for socket operations.
                # Note that this needs to be higher than the idle timeout as otherwise the
                readTimeout = 1815s

                # Specifies the host name as exposed to the outside world.
                # This information is used in the documentation at /kba/EAI5V
                externalHost = ""

                # Specifies the port as exposed to the outside world.
                # This information is used in the documentation at /kba/EAI5V
                externalPort = 0
            }

            # Defines the settings of the built-in FTP server.
            ftp {
                # Specifies the port to listen on. Use 0 to disable the server or 21 to run it on the common FTP port.
                port = 0

                # Specifies the port range to listen on for the data connection in passive mode, use any by default
                # Examples:
                # 2300               only use port 2300 as the passive port
                # 2300-2399          use all ports in the range
                # 2300-              use all ports larger than 2300
                # 2300, 2305, 2400-  use 2300 or 2305 or any port larger than 2400
                passivePorts = ""

                # Specifies the IP address to bind to. Leave empty to use all IP addresses.
                bindAddress = ""

                # Specifies the IP address clients have to connect to in passive mode.
                passiveExternalAddress = ""

                # Specifies the max. login failures before disconnecting.
                maxLoginFailures = 5

                # Specifies the max. number of concurrent clients.
                maxClients = 100

                # Specifies the max. number of threads to utilize.
                maxThreads = 10

                # Specifies the idle timeout for connections.
                idleTimeout = 30m

                # Specifies the max. connections per IP.
                maxConnectionsPerIp = 5

                # Specifies the JKS keystore to use for FTPS.
                keystore = ""

                # Specifies the keystore password.
                keystorePassword = ""

                # Specifies the key alias to use.
                keyAlias = ""

                # Determines if FTPS should be forced or not.
                forceSSL = false

                # Defines the actual TLS ("SSL") protocol to support.
                tlsProtocol = "TLSv1.2"

                # Specifies the host name as exposed to the outside world.
                # This information is used in the documentation at /kba/EAI5V
                externalHost = ""

                # Specifies the port as exposed to the outside world.
                # This information is used in the documentation at /kba/EAI5V
                externalPort = 0
            }
        }

        # Contains a list of known file extensions of server-sided scripting languages. This can be used
        # to determine if the file-extension of a file-url contains the effective filename or if the path
        # is just a script and we need to inspect the ContentDisposition (etc.) to obtain the actual file name
        serverSidedScriptingExtensions = [ "php", "php3", "php4", "php5", "cgi", "asp", "aspx", "jsp", "jspx", "cfm", "cfml", "xml" ]

        # Defines the number of retries for service unavailable responses (HTTP 503).
        # The first retry will wait 200ms with each consequent retry waiting 500ms longer than the previous one.
        retriesForServiceUnavailable = 3
    }

}



security {
    passwords {
        # Contains settings for generated passwords.
        generated {
            # Specifies the length of generated passwords.
            length = 7

            # Specifies the characters to consider when generating passwords.
            # By default, only easily distinguishable characters are considered.
            consideredCharacters = "123456789abcdefghijkmnpqrstuvwz"

            # Specifies for how long generated passwords should be displayed.
            showFor = 5 days
        }

        # Contains requirements and info texts for user defined passwords.
        user {
            categories {
                # Specifies the default password settings.
                default {
                    # Specifies the minimum length of a password.
                    minLength = 4

                    # Specifies if a password must contain both letter and digits.
                    requireLettersAndDigits = false

                    # Specifies if a password must contain upper and lower case letters.
                    requireUpperAndLowerCase = false

                    # Specifies if a password must contain special characters.
                    requireSpecialCharacters = false

                    # Sepcifies the label of the password strength info box for this category.
                    label = ""

                    # Specifies the addition to the label of the password strength info box for this category.
                    labelAddition = ""

                    # Sepcifies the description of the password strength info box for this category.
                    # Use '${minLength}' to insert the minimum length and '${nextMinLength}' to insert the next higher
                    # categories minimum length.
                    description = ""
                }

                # Specifies the texts for the insufficient password category.
                insufficient {
                    label = "$PasswordSettings.insufficient.label"
                    description = "$PasswordSettings.insufficient.description"
                }

                # Specifies the minimum requirements and texts for a password.
                # If these requirements are not met, the password will not be accepted.
                weak {
                    label = "$PasswordSettings.weak.label"
                    description = "$PasswordSettings.weak.description"
                }

                # Specifies the minimum requirements and texts for a password of medium strength.
                fine {
                    minLength = 6
                    requireLettersAndDigits = true
                    label = "$PasswordSettings.fine.label"
                    description = "$PasswordSettings.fine.description"
                }

                # Specifies the minimum requirements and texts for a secure passsword.
                secure {
                    minLength = 6
                    requireLettersAndDigits = true
                    requireSpecialCharacters = true
                    label = "$PasswordSettings.secure.label"
                    labelAddition = "$PasswordSettings.secure.labelAddition"
                    description = "$PasswordSettings.secure.description"
                }
            }
        }
    }

    # Contains the default SAML configuration to be put into the system tenant.
    # This way, the automatic system setup can create a working system without creating a dummy user. Also,
    # if sirius.autoSetup remains true, the SAML configuration can be automatically updates which greatly simplifies
    # certificate rollovers.
    #
    # Note that the fields directly correlate with the SAML settings in TenantData, which can be used as a reference
    # when looking for some documentation.
    system-saml {
        # Specify a number to enforce an external login on a regular basis...
        # externalLoginIntervalDays = 90

        requestIssuerName = ""
        issuerUrl = ""
        issuerIndex = ""
        issuerName = ""
        fingerprint = ""
    }

    scopes.default {
        manager = "tenants"
        system-tenant = "1"
        # Disables API tokens by default, as these are constant and kept in clear-text and thus might impose a security
        # risk in generic setups. We still keep them around, mainly to support legacy systems.
        accept-api-tokens = false
        # Disables hashed API tokens by default. These are a bit less security sensitive, as at least no cleartext value
        # is sent over the wire. Still, the values remain as cleartext in the database and there is no real benefit over
        # simply generating a long and secure password which is properly stored in the database. Still support can be
        # enabled for legacy systems.
        accept-hashed-api-tokens = false
        loginCookieTTL = 90 days
        default-language = "de"
        fallback-language = "en"
        known-languages = ["de", "en", "fr", "nl", "it", "es", "pt", "pl", "cs", "hu", "da", "sv", "fi", "ro", "ru", "sk", "bg", "hr", "no", "sr", "sl", "uk", "lt", "lv"]
        display-languages = ["de", "en", "fr", "nl", "it", "es", "pt", "pl", "cs", "hu", "da", "sv", "fi", "ro", "ru", "sk", "bg", "hr", "no", "sr", "sl", "uk", "lt", "lv"]
    }

    permissions {
        permission-manage-tenants       : "Required to manage tenants of the system"
        permission-manage-system-users  : "Required to manage user accounts of the system tenant"
        permission-manage-user-accounts : "Required to manage user accounts"
        permission-delete-user-accounts : "Required to delete user accounts"
        permission-manage-code-lists    : "Required to manage code lists"
        permission-system-protocols     : "Required to view protocols like logs, errors, mails, all audit logs"
        permission-system-cluster       : "Required to view and manage the cluster state"
        permission-audit-logs           : "Required to view audit logs for the own tenant"
        permission-system-journal       : "Required to view the system journal"
        permission-select-tenant        : "Required to switch to another tenant"
        permission-select-user-account  : "Required to switch to another user"
        permission-execute-jobs         : "Required to execute jobs"
        permission-manage-scheduler     : "Required to plan and edit scheduled jobs"
        permission-system-scripting     : "Required to execute administrator level scripts on all nodes of the cluster"
        permission-view-files           : "Required to view files stored in the system"
        permission-view-processes       : "Required to view processes"
        permission-view-audit-log       : "Required to view the audit log"
        permission-unlock-files         : "Required to unlock read-only files"

        permission-open-search          : "Required to use the open-search"

        feature-bypass-process-log-limits : "Required to bypass log limits of processes via a job parameter"

        # legacy storage framework...
        permission-manage-files         : "Required to manage well known buckets in the storage system"
        permission-manage-admin-files   : "Required to access administrative buckets in the storage system"
        # end of legacy...

        permission-manage-processes     : "Required to view processes of other users within the same tenant"
        permission-manage-all-processes : "Required to view processes of all users and tenants"
        permission-view-rate-limits     : "Required to view tenant-wide rate limits"
        permission-control-disaster-mode: "Required to enable or disable the maintenance / disaster mode"
        feature-user-account-config     : "Required (most probably as tenant permission) to provide custom configurations for user accounts"
    }

    # Defines all roles known by the system. This can be redefined by the application.
    roles = ${?security.roles} ["system-administrator", "user-administrator", "administrator", "jobs-manager", "jobs-execution", "file-manager"]

    # Defines all sub scopes known by the system. This can be redefined by the application.
    subScopes = ${?security.subScopes} ["ui", "api", "vfs", "empty"]

    # Defines all tenant permissions (aka features). This also can be redefined by the application.
    tenantPermissions = ${?security.tenantPermissions} ["feature-bypass-process-log-limits"]

    profiles {

        flag-logged-in {
            priority = 40
            permission-view-files = true
            permission-view-processes = true
            permission-view-audit-log = true
            permission-open-search = true
        }

        # If a user belongs to the system tenant, we set the member&affiliate flag
        flag-system-tenant {
            priority = 50
            flag-system-tenant-member = true
            flag-system-tenant-affiliate = true
        }

        # Grants the system administration and system user management permissions and flags to the role
        # "system-administrator"...
        "system-administrator+flag-system-tenant-affiliate" {
            priority = 60
            flag-system-administrator = true
            permission-manage-system-users = true
        }

        flag-system-administrator {
            priority = 110
            permission-manage-tenants = true
            permission-manage-code-lists = true
            permission-system-protocols = true
            permission-system-cluster = true
            permission-system-journal = true
            permission-system-console = true
            permission-system-timing = true
            permission-system-notify-state = true
            permission-system-load = true
            permission-manage-all-processes = true
            permission-view-rate-limits = true
            permission-system-tags = true
            permission-system-tags-state = true
            permission-delete-user-accounts = true
            permission-control-disaster-mode = true
            permission-system-scripting = true
            permission-unlock-files = true

            # By default we only permit sys admins to change the configs of any user in any
            # tenant as this is quite a specific and dangerous task...
            feature-user-account-config = true
            permission-system-health-api = true
        }

        user-administrator {
            priority = 120
            permission-manage-user-accounts = true
            permission-select-user-account = true
        }

        "user-administrator+flag-system-tenant-affiliate" {
            permission-manage-tenants = true
            permission-select-tenant = true
        }

        administrator {
            priority = 130
            permission-select-tenant = true
            permission-execute-jobs = true
            permission-manage-scheduler = true
            permission-view-scope-default-config = true
            permission-manage-files = true
            permission-manage-admin-files = true
            permission-system-audit-logs = true
            permission-manage-processes = true
            permission-view-rate-limits = true
        }

        jobs-manager {
            priority = 140
            permission-execute-jobs = true
            permission-manage-scheduler = true
            permission-manage-processes = true
        }

        jobs-execution {
            priority = 150
            permission-execute-jobs = true
        }

        # legacy storage framework...
        file-manager {
            priority = 160
            permission-manage-files = true
        }
    }

    # defines the packages and upgrades for the different scopes
    packages {

        #The 'tenant' scope used by the tenants framework (TenantUserManager etc.)
        tenant {
            # the packages of the scope
            packages = []

            # the upgrades of the scope
            upgrades = []
        }

        # indicates which permissions are needed to show a specific permission for a user
        # permissions which have no requirements can be omitted
        # have to be key-value pairs, in which the key is the permission in question and the value is the required permission
        required-permissions-for-permission {
            system-administrator : "flag-system-tenant"
        }
    }

    # Permits to enable access to /system/query/api, disabled by default
    query-api {
        enabled = false
    }
}

# Specifies cache sizes used by the biz platform
cache {

    tenants-users {
        maxSize = 100
        ttl = 1 hour
    }

    tenants-roles {
        maxSize = 100
        ttl = 1 hour
    }

    tenants-children {
        maxSize = 256
        ttl = 1 hour
    }

    tenants-tenants {
        maxSize = 512
        ttl = 1 hour
    }

    tenants-configs {
        maxSize = 100
        ttl = 1 hour
    }

    storage-directories {
        maxSize = 8192
        ttl = 1 hour
    }

    storage-filenames {
        maxSize = 8192
        ttl = 1 hour
    }

    storage-physical-keys {
        maxSize = 16384
        ttl = 1 hour
    }

    storage-paths {
        maxSize = 8192
        ttl = 1 hour
    }

    # Legacy
    storage-object-metadata {
        maxSize = 16384
        ttl = 1 hour
    }

    # Legacy
    virtual-objects {
        maxSize = 16384
        ttl = 1 hour
    }

    processes-first-level {
        maxSize = 128
        ttl = 10 seconds
    }

    processes-second-level {
        maxSize = 256
        ttl = 10 minutes
    }

    standby-processes {
        maxSize = 1024
        ttl = 1 hour
    }

    codelists-values {
        maxSize = 4096
        ttl = 1 hour
    }

    codelists-reverse-lookup {
        maxSize = 4096
        ttl = 1 hour
    }

    objectstores-buckets {
        maxSize = 128
        ttl = 1 hour
    }

    metrics {
        maxSize = 8192
        ttl = 1 hour
    }

    jupiter-local-small {
        maxSize = 4096
        ttl = 1 hour
    }

    jupiter-local-large {
        maxSize = 512
        ttl = 1 hour
    }

    tycho-academy-videos {
        maxSize = 64
        ttl = 20 hours
    }
}

# By default we use the smart lock manager. This detects the presence of redis and uses cluster-wide locks
# or otherwise uses fast local locks within the JVM. to enforce local locks, use "java".
# Another approach for clusters without Redis is using an SQL Database to implement locks distributed locks
# which is available via "sql" (SQLLockManager).
locks.manager = "smart"

# Determines how "Sequences" are stored and computed. By default a "smart" strategy is used which either
# checks if a "sql" database or a "mongo" database is ready and picks the right strategy. If both are
# available the effective startegy can be determined by setting an explicit value here.
sequences.strategy = "smart"

# Provides some aliases to simplify importing user accounts
importer.aliases {
    sqluseraccount {
        userAccountData_email: [ "e-mail", "EMail" ]
        userAccountData_externalLoginRequired: [ "externalLoginRequired" ]
        userAccountData_person_title: [ "title" ]
        userAccountData_person_salutation: [ "salutation" ]
        userAccountData_person_firstname: [ "firstname" ]
        userAccountData_person_lastname: [ "lastname" ]
        userAccountData_login_username: [ "username", "user" ]
        userAccountData_login_generatedPassword: [ "password", "$LoginData.password" ]
        userAccountData_login_accountLocked: [ "locked" ]
        userAccountData_permissions_permissions: [ "roles", "permissions" ]
    }

    mongouseraccount {
        userAccountData_email: [ "e-mail", "EMail" ]
        userAccountData_externalLoginRequired: [ "externalLoginRequired" ]
        userAccountData_person_title: [ "title" ]
        userAccountData_person_salutation: [ "salutation" ]
        userAccountData_person_firstname: [ "firstname" ]
        userAccountData_person_lastname: [ "lastname" ]
        userAccountData_login_username: [ "username", "user" ]
        userAccountData_login_generatedPassword: [ "password", "$LoginData.password" ]
        userAccountData_login_accountLocked: [ "locked", "$LoginData.accountLocked" ]
        userAccountData_permissions_permissions: [ "roles", "permissions", "$PermissionData.permissions" ]
    }

    sqlcodelistentry {
        codeListEntryData_code: [ "code" ]
        codeListEntryData_priority: [ "priority" ]
        codeListEntryData_value: [ "value" ]
        codeListEntryData_additionalValue: [ "additionalValue" ]
        codeListEntryData_description: [ "description" ]
    }

    mongocodelistentry {
        codeListEntryData_code: [ "code" ]
        codeListEntryData_priority: [ "priority" ]
        codeListEntryData_value: [ "value" ]
        codeListEntryData_additionalValue: [ "additionalValue" ]
        codeListEntryData_description: [ "description" ]
    }
}

# Determines which databases can be directly queried via /system/sql
# By default we allow to access the system database and clickhouse, which is the statistics database.
# Note that if a database is listed here, but not present, it will be ignored.
jdbc.selectableDatabases = [ system, clickhouse ]

# Contains settings which are used by the analytics and metrics sub-system.
analytics {
    # Contains settings regarding the metric computation for UserAccounts...^
    user-accounts {
        # Contains the number of days for which the user-activity metric is computed. This metric is the
        # percentage of days of the given period in which the user was seen.
        observationPeriodDays = 45

        # Contains the minimal number of days on which the user has to been in the above defined period
        # so that the "active-user" flag is toggled.
        minDaysForActiveUsers = 1

        # Contains the minimal number of days on which the user has to been in the above defined period
        # so that the "frequent-user" flag is toggled.
        minDaysForFrequentUsers = 4

        # Contains the minimal required education level in percent in order to toggle the "academy-user"
        # flag.
        minEducationLevel = 10
    }
}

# Contains a map of condition flags, which decide whether a MultiLanguageString brings true
# i18n capabilities or if a fallback to a single field should by used, as i18n support isn't needed.
i18n {
    # By default, i18n support for code-lists is enabled
    code-lists = true
}

redis {
    pools {
        # Provides defaults for a Jupiter uplink which is based on the Redis RESP protocol but
        # runs on port 2410.
        jupiter {
            port = 2410
            enableClientInfo = false
        }
    }
}

# Configures the feed from which updates / news for the Tycho UI are loaded.
tycho.updates {

    # Specifies the ATOM feed to fetch news from
    feedUrl = ""

    # Specifies the categories of which one has to be present in an item to be accepted. If left empty, all
    # items will be accepted.
    triggerCategories = []

    # Specifies the categories which mark important updates. Users are notified about these updates if they don't
    # read them. Note that one of the categories must match in order to mark the update as important.
    importantCategories = []
}

# Provides config settings for the onboarding / video academy framework.
tycho.onboarding {

    # Lists all academies which are made available to users of the tenants framework, which
    # are generally the users of the main / backend system.
    tenants-academies = []

    # Specifies an extension per academy supported by this system.
    academies {
        # Provides a default academy based on the OXOMIAcademyProvider.
        default {
            # Contains the name of the AcademyProvider to use
            provider = "oxomi"

            # Contains all settings which specify how oxomi is being accessed to fetch videos...
            host = "oxomi.com"
            user = ""
            portal = ""
            sharedSecret = ""
            academyId = ""
        }
    }
}

# Specifies the settings for Jupiter connectors.
jupiter {

    # Determines if this node performs an automatic update of the repository and configuration for all Jupiter
    # instances. Note that on many node clusters this should only be enabled on one or two nodes.
    automaticUpdate = true

    # Lists for which instances the config should be updated automatically.
    updateConfig = [ "jupiter" ]

    # Lists for which instances the repository should be synced automatically.
    syncRepository = [ "jupiter" ]

    repository {
        # Contains the configurations to S3 buckets which are synchronized to the Jupiter repository.
        uplinks {
            default {
                # Contains the name of the s3 store which hosts the bucket.
                store = ""

                # Contains the name of the bucket to sync.
                bucket = ""

                # Contains the name of the bucket to sync when Sirius is running in DEV, STAGING or TEST mode.
                # This can be used if the underlying datasource provides a preview location to test unreleased
                # master data.
                testBucket = ""

                # Contains a list of path prefixes which will be ignored when syncing.
                # If the repository contains a "folder" (keys with a path prefix) named "media/.."
                # which should not be transferred to Jupiter, this path could be listed below.
                ignoredPaths = []

                # Contains a list of namespaces out of which at least one has to be active for the uplink
                # repository to be synced. If this list is empty, no check is performed.
                requiredNamespaces = []
            }
        }

        # Contains the name of the layer 2 space which is also synchronized into the Jupiter repository.
        localSpaceName = ""

        # Contains the hostname which is used to generate download URLs send to Jupiter when synchronizing
        # the local storage space
        hostUrl = "http://localhost:9000"
    }

    # Determines a fallback instance for each connector for HA scenarios.
    ha {
        # jupiter: fallback
    }

    settings {
        # Specifies de default settings for all instances.
        default {

            # Specifies the settings for each LRU cache.
            caches {
                # Provides an example block for a LRU cache.
                # Note that most settings are defined as string, so that Jupiter itself
                # can parse them (rather than letting typesafe config parse the values...).
                # example {
                #    # Specifies the maximal number of keys to cache. Note that 0 will disable the cache.
                #    size: 0
                #
                #    # Specifies the maximal amount of memory to consume.
                #    maxMemory: "1g"
                #
                #    # Specifies the soft timeout (after which a re-computation will be requested)
                #    softTTL: "15m"
                #
                #    # Specifies the hard timeout after which stale entries are ignored.
                #    hardTTL: "1d"
                #
                #    # One a node was instructed to re-compute a value when using XGET, this is the interval
                #    # for which no other node (accessing the same key) will be instructed to compute a value.
                #    refreshInterval: "30s"
                # }
            }
        }

        # Provides settings for the jupiter default instance.
        jupiter {
        }

    }
}

user-assistant {
    academy-track {
        # Contains a map URI -> academy track id, to link to the given track in the UI, e.g.:
        # "/data-explorer": "12345"
    }

    academy-video {
        # Contains a map URI -> academy video code, to link to the given video in the UI, e.g.:
        # "/data-explorer": "12345"
    }

    kba {
        # Contains a map URI -> knowledge base article code, to link to the given KBA in the UI, e.g.:
        # "/data-explorer": "ABKJE"
    }
}

tagliatelle {
    taglib {
        k = "Knowledge Base"
    }
}

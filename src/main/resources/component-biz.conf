product {
    modules {
        sirius-biz {
            version = "${project.version}"
            build = "${build.number}"
            date = "${timestamp}"
            vcs = "${build.vcs.number}"
        }
    }
}

# sirius-biz provides a multitude of frameworks for different use-cases
# and implements them using different databases. Therefore by default
# all frameworks are disabled and have to be enabled by the application.
sirius.frameworks {

    # User-manager which supports multi-tenant applications
    biz.tenants = false

    # Enables the JDBC storage layer for tenants.
    biz.tenants-jdbc = false

    # Enables the MongoDB storage layer for tenants.
    biz.tenants-mongo = false

    # A framework for letting a user map codes to text values
    biz.code-lists = false

    # Enables the JDBC storage layer for code-lists.
    biz.code-lists-jdbc = false

    # Enables the MongoDB storage layer for code-lists.
    biz.code-lists-mongo = false

    # Utilizes Elasticsearch to store all recorded logs, incidents,
    # audit logs and mails
    biz.protocols = false

    # Provides a change log for all entities which include a JournalData
    biz.journal = false

    # Provides an ID generator which can either use MongoDB or JDBC
    # to generate sequences of unique IDs
    biz.sequences = false

    # Provides distributed locks based on either the JVM, Redis or JDBC
    biz.locks = false

    # Provides a object store like API for storing files. This can either use
    # the file system or other storage facilities.
    biz.storage = false

    # Provides the metadata storage for the layer 2 by storing all metadate in a JDBC datasource
    biz.storage-blob-jdbc = false

    # Provides the metadata storage for the layer 2 by storing all metadate in MongoDB
    biz.storage-blob-mongo = false

    # Provides a replication system for stored objects by using a JDBC datasource as
    # metadata repository.
    biz.storage-replication-jdbc = false

    # Provides a replication system for stored objects by using a MongoDB as
    # metadata repository.
    biz.storage-replication-mongo = false

    # Provides a rate-limiting / firewall which is either based on Redis.
    biz.isenguard = false

    # Provides a framework to record and visualize the output of background processes.
    biz.processes = false

    # Provides a framework to execute all kinds of application defined jobs.
    biz.jobs = false

    # Provides a framework store the job presets in a JDBC database.
    biz.job-presets-jdbc = false

    # Provides a framework store the job presets in a MongoDB database.
    biz.job-presets-mongo = false

    # Provides a framework to execute planned / scheduled tasks which are stored in a JDBC database.
    biz.scheduler-jdbc = false

    # Provides a framework to execute planned / scheduled tasks which are stored in a MongoDB database.
    biz.scheduler-mongo = false

    # Provides a storage option to place execution flags in a JDBC database.
    biz.analytics-execution-flags-jdbc = false

    # Provides a storage option to place execution flags in a MongoDB.
    biz.analytics-execution-flags-mongo = false

    # Provides a storage option for metrics in a JDBC database.
    biz.analytics-metrics-jdbc = false

    # Provides a storage option for in a MongoDB.
    biz.analytics-metrics-mongo = false
}

# Place the local address of the node here, i.e. http://192.168.0.1
# If no address is given, we use the local address determined by the system
# (using: InetAddress.getLocalHost().getHostAddress()) - however, in some
# environments like Docker, this might yield an inappropriate address.
#
# This address has to be reachable from all other cluster nodes.
sirius.nodeAddress = ""

# Contains a local token which is used by the Cluster controller so that some APIs can be invoked without
# further authentication.
sirius.clusterToken = ""

# Contains settings for the built-in firewall and rate-limiting facility
isenguard {
    # Determines which limiter is used. By default we use a "smart" strategy,
    # which uses "redis" is available and otherwise switches to the "noop" limiter.
    limiter = "smart"

    # If the "Redis" limiter is used, the given redis database is used to store
    # the counters and blocked IPs. By default we use the "system" database,
    # which is the default redis.
    redisName = "system"

    # Contains an interval and limit per interval for each realm.
    # Note that the realm "http" is used to limit all notable
    # HTTP calls.
    limit {
        # Specifies the defaults for all realms unless noted otherwise.
        # By default IsenGuard is turned off.
        default {
            # Defines the check interval
            interval = 10m

            # Defines the max number of occurrences within the given inverval
            limit = 0

            # Declares which the "scope" value of this realm will be.
            # There are three standard types:
            # - ip:     Limiting by ip address
            # - tenant: Limiting by tenant id
            # - user:   Limiting by user id
            #
            # For all other kinds of realms, the type "custom" can be used.
            # Also, additional frameworks might define more types.
            type = "custom"
        }

        # Specifies the constraints for all HTTP requests. By default this
        # is turned off, as there is no way of knowing the usage pattern
        # of a specific application.
        http {
            interval = 0
            limit = 0
            type = "ip"
        }

        # Specifies the constraints for negative AuditLog events (wrong password etc).
        # Once this limit is hit, the calling IP will be blocked for ten minutes.
        security {
            interval = 2m
            limit = 50
            type = "ip"
        }
    }

}

health.limits {
    # If there is any lock held, we will report this - but there is no
    # sane limit how many locks can be considered healthy / unhealty
    locks-count.gray  = 1
    locks-count.warning  = 0
    locks-count.error = 0

    # We start to warn as soon as we encounter one long running lock
    # (held for at least 30min). As this can still be quite alright
    # we do not consider this critical (red)
    locks-long-running.gray = 0
    locks-long-running.warning = 1
    locks-long-running.error = 0

    # Monitors the utilization of the events buffer which is used by the
    # EventRecorder to permit batch inserts of recorded events into Clickhouse
    events-buffer-usage.gray = 0
    events-buffer-usage.warning = 80
    events-buffer-usage.error = 99

    # Number of active tasks (remains gray when zero). There is no limit
    # to warn about, as the number can be specified in the system configuration.
    active-distributed-tasks.gray = 1
    active-distributed-tasks.warning = 0
    active-distributed-tasks.error = 0
}

async {
    # Defines the maximal number of concurrent tasks executed for the
    # Distributed Tasks framework.
    executor.distributed-tasks {
        poolSize = 8

        # Having a queue would be pointless, as the WorkLoaderLoop only
        # tries to keep the available executors running but will not
        # schedule additional work.
        queueLength = 0
    }

    # Interctive jobs should actually execute quite instantly. Therefore
    # we only permit a low parallelism but a certain queue length for peak loads.
    executor.interactive-jobs {
        poolSize = 2
        queueLength = 100
    }

    distributed {
        # Configures the nature of the queues used to distribute tasks.
        queues {
            # Each queue needs to suppliy the following settings
            # example {
                # Contains the concurrency token to control node-local parallelism
                # concurrencyToken = SomeToken

                # Determines if the queue is prioritized or a FIFO queue
                # prioritized = false

                # For prioritized queues the penalty should approximately be
                # equal to the expected runtime of an average task. This time is
                # used to compute the effective execution priority once a task is
                # scheduled.
                # penaltyTime = 1 minute
            # }

            # Defines the queue used by the MetricsGuaranteedSchedulerExecutor when scheduling metrics.
            metrics-scheduler {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the MetricsGuaranteedBatchExecutor when executing batches of metrics tasks.
            metrics-batch {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the MetricsBestEffortSchedulerExecutor and MetricsBestEffortBatchExecutor
            # when scheduling and executing batches of "best effort" metrics tasks.
            #
            # These tasks are only scheduled if the queue is empty. Therefore, if the system is overloaded it may
            # skip some of the "best effort" tasks - but it will never skip a guaranteed task.
            metrics-best-effort {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the CheckBatchExecutor to execute DailyChecks and ChangeChecks.
            checks {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the DefaultBatchProcessTaskExecutor for miscellaneous jobs.
            jobs {
                # We use a simple token here so that the number of parallel jobs can be specified
                concurrencyToken = "small-jobs"

                # The queue will be prioritized
                prioritized = true

                # As we have no idea what the average runtime of a job in this queue might be, we default the penalty
                # time to one hour.
                penaltyTime = 1h
            }

            # Defines the queue used by the ImportBatchProcessTaskExecutor for import jobs.
            import-jobs {
                concurrencyToken = "large-jobs"
                prioritized = true
                penaltyTime = 2h
            }

            # Defines the queue used by the ExportBatchProcessTaskExecutor for export jobs.
            export-jobs {
                concurrencyToken = "large-jobs"
                prioritized = true
                penaltyTime = 30m
            }

            # Defines the queue used by the ReportBatchProcessTaskExecutor for report jobs.
            report-jobs {
                concurrencyToken = "small-jobs"
                prioritized = true
                penaltyTime = 30m
            }

            # Defines the queue used by the CheckBatchProcessTaskExecutor for check jobs.
            check-jobs {
                concurrencyToken = "small-jobs"
                prioritized = true
                penaltyTime = 1h
            }

            # Provides the config for the queue which is used by the storage layer 1 to perform
            # replication tasks.
            storage-layer1-replication {
                concurrencyToken = "system"
                prioritized = false
            }
        }

        # Configures concurrency tokens which are semaphores on each node and
        # control local parallelism. Note that a single token can be shared by
        # multiple queues.
        concurrency {
            # Specifies the maximal number of parallel small jobs.
            small-jobs = 4

            # Specifies the maximal number of parallel large jobs.
            large-jobs = 2

            # Specifies the maximal number of parallel analytical tasks.
            analytics = 2

            # Specifies the maximal number of parallel system tasks to perform.
            system = 1
        }
    }
}

# Provides a cluster wide controller for executing background jobs.
# NeighborhoodWatch uses Redis locks and timestamps to control the
# execution of background jobs across a cluster of nodes.
# Per job one for the following settings can be set:
# LOCAL    - the jobs runs on this node independently of the cluster
# CLUSTER  - the job may run on this node, but only on one node within the cluster at once
# DISABLED - the job is disabled on this node
orchestration {
    loop-elastic-auto-batch = LOCAL
    loop-event-processor = LOCAL
    loop-delay-line = LOCAL
    loop-distributed-tasks-work-loader = LOCAL
    loop-redis-limiter-cleanup = CLUSTER
    loop-job-scheduler = CLUSTER
    loop-storage-layer1-replication = CLUSTER
    loop-storage-layer2-delete = CLUSTER
    loop-storage-layer2-touch-writeback = LOCAL
    task-analytical-engine = CLUSTER
    task-end-of-day = CLUSTER

    # TODO remove once stroage is migrated...
    task-storage-cleaner = CLUSTER
}



timer.daily {

    # Determines when protocols and journals are purged based on the given settings...
    protocols-cleaner = 2

    # Determines when old execution flags are purged...
    delete-execution-flags = 4

    # TODO kill when storage has been migrated:
    storage-cleaner = 3

    # Determines when outdated files in the blob storage system are purged...
    storage-layer2-cleaner = 3

    # Determines when analytical tasks are scheduled...
    analytical-engine = 23

    # Determines when expires process logs are removed...
    cleanup-processes = 2

}

# Controls the storage duration of protocol entries
protocols {
    keep-logs = 30 days
    keep-incidents = 30 days
    keep-mails = 365 days
    keep-journal = 1000 days
    keep-neutral-audit-logs = 30 days
    keep-negative-audit-logs = 180 days

    # We limit messages to this length when storing them in ES as otherwise we might run into
    # low memory conditions or overload the AutoBatchLoop and ES. Note that the logging system which
    # captures stdout will still receive the full message. Still consider to limit yourself to
    # sane and digestable log messages.
    maxLogMessageLength = 116384
}

# The audit log can write additional logs to the system log.
# This can be used to preserve the audit logs in an external archive with a
# potentially longer storage period. By default this is disabled to not jam
# the system logs.
logging.audit = OFF

# Specifies the secret used to sign internal URLs for specific entities...
# An empty secret signals, that a new (local) secret es generated during startup...
controller.secret = ""

jobs.categories {
    import {
        label = "$JobCategory.import"
        priority = 100
        icon = "fa-upload"
    }
    export {
        label = "$JobCategory.export"
        priority = 200
        icon = "fa-download"
    }
    check {
        label = "$JobCategory.check"
        priority = 300
        icon = "fa-check-square-o"
    }
    report {
        label = "$JobCategory.report"
        priority = 400
        icon = "fa-line-chart"
    }
    misc {
        label = "$JobCategory.misc"
        priority = 500
        icon = "fa-cogs"
    }
}

# Defines default code lists known to the system.
code-lists {
    default {
        # Determines if unknown codes should be recorded automatically so that
        # they can be managed in the admin UI
        autofill = true

        # Determines if a single code list is shared accross all tenants. These code lists
        # are owned by the system tenant and should have "autofill" turned off for obvious
        # reasons.
        global = false

    }

    # Defines the list of salutations.
    salutations {
        name = "Salutations"
        description = "Contains all salutations known to the system"
    }

    # Defines the list of countries.
    countries {
        name = "Countries"
        description = "Contains all countries known to the system. A RegEx can be supplied as additional value which is used to verify ZIP codes"
    }
}

# Contains the configuration of the file / object store system.
storage {

    # In order to support access control over multiple nodes, a shared secret must be placed here
    # (in the instance.conf) so that each node can verify any links / downloads geneared by other
    # nodes. If this remains empty, each node will generate its own internal secret.
    # sharedSecret = "VERY LONG AND SECURE SECRET"

    # Defines the base directory when storing buckets in disk.
    baseDir = "data/storage"

    # If using ImageMagick, consider a command like:
    # "convert ${src} -resize ${width}x${height}> -quiet -quality 89 -format ${imageFormat} -strip -colorspace RGB -background white ${extend} -flatten ${dest}"
    conversionCommand = ""

    # Option for the conversion command to extend the image to a minimum size
    extendOption = "-gravity center -extent ${extendWidth}x${extendHeight}<"

    # Defines all buckets known to the system.
    buckets {
        default {
            # Defines the permission required to view the bucket in the management UI.
            permission = "permission-manage-files"

            # Determines if an object (file) can be created via the management UI.
            canCreate = false

            # Determines if an object (file) can be edited via the management UI.
            canEdit = false

            # Determines whether a search in a bucket should always use a like constraint.
            alwaysUseLikeSearch = false

            # Determines if an object (file) can be deleted via the management UI.
            canDelete = false

            # Determines if objects are automatically removed after N days. 0 means disabled.
            deleteFilesAfterDays = 0

            # Determines the storage engine used for the bucket.
            engine = "fs"

            # Whether uses of the bucket should be logged into the deprecated log if it's log level is FINE.
            logAsDeprecated = false
        }

        # A work directory / bucket is provided per tenant and can be used to in- and output files.
        # This is also visible in the built-in virtual file system (FTP server) to upload and download files.
        # To limit the number of files in this directory, old files (older than 30 days) are automatically removed.
        # Therefore this should not be used for permanent storage.
        work {
            canCreate = true
            canEdit = true
            canDelete = true
            deleteFilesAfterDays = 30
        }

        # Provides a temporary storage space which is automatically maintained (files are deleted after 30 days).
        tmp {
            permission = "permission-manage-admin-files"
            canCreate = true
            canEdit = true
            canDelete = true
            deleteFilesAfterDays = 30
        }

        # Defines storage for versioned files
        versioned-files {
            canCreate = false
            canEdit = false
            canDelete = false

            # number of versions kept from one versioned file
            # setting this number to 0 will keep all versions
            maxNumberOfVersions = 50
        }
    }

}

# Provides credentials for the S3 compatible stores managed by ObjectStores.
s3 {
    stores {
        # Provides the default configuration shared by all stores.
        default {
            accessKey = ""
            secretKey = ""
            endPoint = ""
            bucketSuffix = ""
            pathStyleAccess = true

            # Specifies the signer to use. Leave empty to use the standard signer of the
            # current AWS SDK.
            signer = ""
            # Use the following setting for CEPH stores:
            # signer = "S3SignerType"
        }

        # By default a "system" store is used if no other name is given.
        # An application should provide a configuration for this store if ObjectStores are used.
        system {

        }
    }
}

# Contains settings for the virtual file system.
storage {

    # Contains settings for the physical storage layer which either persists objects onto
    # disk or into a S3 compatible store.
    layer1 {

        # Defines the replication settings
        replication {
            # Defines the number of replication tasks in a batch.
            batchSize = 25

            # Defines the max number of batches to queue.
            maxBatches = 100

            # Defines the duration for which a delete is delayed.
            replicateDeleteDelay = 45d

            # Defines the duration for which an update is delayed.
            replicateUpdateDelay = 5m

            # Contains the duration for which a retry of a failed replication task is delayed.
            retryReplicationDelay = 90m

            # Determines the max number of replication attempts before a task is considered as "failed".
            maxReplicationAttempts = 5
        }

        # Enumerates the physical storage spaces known to the system.
        spaces {
            # Defines the defaults shared by all spaces.
            default {
                # Defines the engine to use. This can be either "s3" or "fs".
                engine = "s3"

                # Defines the s3 store to use (defined in s3.stores)
                store = "system"

                # For the "fs" engine a baseDir can be defined where a spaces are placed unless configured
                # otherwise (see below).
                baseDir = "data/storage"

                # Defines an effective path to store object into when using the "fs" engine.
                # If left empty, the "baseDir" + space name is used as path.
                path = ""

                # Defines the compression level to apply when storing objects.
                # Possible values are: off, fast, default, best (see CompressionLevel)
                compression = "off"

                # Selects the cipher factory to use for encrypting and descripting data.
                # No encryption will be used when left empty.
                # Possible values are any name of a CipherFactory e.g. "aes256" (AES256CipherFactory)
                cipher = ""

                # Specifies the passphrase used to initialize the cipher provider
                # (e.g. used by the AES256CipherFactory).
                passphrase = ""
            }

            # Defines the layer 1 settings for the work space
            work {
                # This is empty as we stick with the default settings.
            }

            # Defines the layer 1 settings for the temporary storage space
            tmp {
                # This is empty as we stick with the default settings.
            }

            # Defines the layer 1 settings of the storage space used for process files.
            processes {
                # This is empty as we stick with the default settings.
            }
        }
    }

    # Contains settings for the blob storage system.
    layer2 {
        # Controls the conversion settings used by the BlobStorageSpace to generate variants of a blob.
        conversion {

            # Determines if conversions are enabled on this node
            enabled = true

            # Contains a list of hostnames or IP addresses to which a conversion can be delegated by
            # forwarding a request.
            hosts = []

            variants {
                # Provides a simple variant which uses the identity converter. This is mostly used as a test
                # of the conversion framework as it doesn't provide any actual benefit.
                identity {
                    # Names the converter (see config block below) to use.
                    converter = "identity"
                }
            }

            converters {
                # The identity converter doesn't required any configuration at all.
                identity {
                    # Contains the converter (actually the name of the ConverterFactory) to use.
                    type = "identity"
                }
            }
        }

        spaces {
            default {
                # Defines the permission which is required to browse and read the space in the layer 3. Note that
                # this is not enforced by direct (layer 2) access.
                readPermission = "enabled"

                # Defines the permission which is required to write / modify blobs in the space via the layer 3.
                # Note that this is not enforced by direct (layer 2) access.
                writePermission = "enabled"

                # Contains a custom base url (hostname) like "https://my-custom-host.io" which will be
                # used when creating delivery URLs for this space.
                baseUrl = ""

                # Determines if the file system in this space is case sensitive (false) or case insensitive
                # (true). By default, we use a case sensitive file system as known from the UNIX world.
                useNormalizedNames = false

                # Determines the retention period in days.
                # If non-zero all blobs in this space will be deleted after the given period
                # (starting from their last modified date or last touched date).
                retentionDays = 0

                # Determines if touch tracking (keeping the lastTouched timestamp up to date) is enabled or not.
                # If a space is heavily used, it might be necessary to turn this off (especially of the generated
                # data isn't used).
                touchTracking = true
            }

            # Defines a work space which is shared by all users of a tenant and can be used to provide
            # an input and output to jobs etc.
            work {
                description = "$BlobStorageSpace.work.description"
                retentionDays = 30
            }

            # Provides a temporary storage location which will be cleaned up every once in a while.
            # This is normally invisible and can only be used to resolve temporary files via the TmpRoot.
            tmp {
                readPermission = "disabled"
                writePermission = "disabled"
                retentionDays = 10
            }

            # This space is used to store files which are attached to Processes
            processes {
                readPermission = "disabled"
            }
        }
    }

    layer3 {

        # Defines uplinks available to all tenants.
        roots {
            # Defines a file system uplink which makes part of the local file system visible to the VFS
            # fs {
            #    # Contains the name of the virtual directory
            #    name = "fs"
            #
            #    # Contains an optional short description of the mount point
            #    description = ""
            #
            #    # Defines the type of the uplink
            #    type = "fs"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Contains the base path to mount
            #    basePath = "/data/somewhere"
            # }

            # Defines a file system uplink which mounts a CIFS share into the VFS
            # cifs {
            #     # Contains the name of the virtual directory
            #    name = "MyShare"
            #    # Contains an optional short description of the mount point
            #    description = "Some words of caution"
            #
            #    # Defines the type of the uplink
            #    type = "cifs"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Contains the smb url to the share
            #    url = "smb://my.samba.share/path/"
            #
            #    # Contains the domain to which the user belongs which is used to authentication
            #    domain = "my-domain.local"
            #
            #    # Contains the user used to mount the share
            #    user = ""
            #
            #    # Contains the password of the user
            #    password = ""
            # }

            # Defines a file system uplink which mounts a remote SFTP server into the VFS
            # sftp {
            #     # Contains the name of the virtual directory
            #    name = "MyShare"
            #
            #    # Contains an optional short description of the mount point
            #    description = "Some words of caution"
            #
            #    # Defines the type of the uplink
            #    type = "sftp"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Determines the host to connect to
            #    host = "hostname"
            #
            #    # Determines the port to use (Default is 22)
            #    port = 22
            #
            #    # Determines the uername used to login
            #    user = "username"
            #
            #    # Determines the password used to authenticate
            #    password = "secretPassword"
            #
            #    # Contains the base path to mount (Default is "/")
            #    basePath = "/data/somewhere"
            #
            #    # Specifies the maximal number of idle connections kept in the pool (Default is 1)
            #    # maxIdle = 1
            #
            #    # Specifies the maximal number of active connections (Default is 5)
            #    # maxActive = 5
            #
            #    # Specifies the connect timeout in milliseconds (Default is 10000)
            #    # connectTimeoutMillis = 10000
            #
            #    # Specifies the read / IO timeout in milliseconds (Default is 10000)
            #    # readTimeoutMillis = 10000
            #
            #    # Specifies the idle timeout in milliseconds. This is the maximal duration for with an unused
            #    # connection is kept open. (Default is 600_000 = 600s = 10 min)
            #    # idleTimeoutMillis = 600000
            #
            #    # Specifies the wait time in milliseconds. If the connection pool is exhaused, the is the maximal
            #    # duration we wait for a connection to become available. (Default is 10000)
            #    # connectTimeoutMillis = 10000
            #
            # }
            #
            # Defines a file system uplink which mounts a remote FTP server into the VFS
            # ftp {
            #     # Contains the name of the virtual directory
            #    name = "MyShare"
            #    # Contains an optional short description of the mount point
            #    description = "Some words of caution"
            #
            #    # Defines the type of the uplink
            #    type = "ftp"
            #
            #    # Determines if the directory is mounted readonly (Default is false)
            #    readonly = true
            #
            #    # Determines the host to connect to
            #    host = "hostname"
            #
            #    # Determines the port to use (Default is 21)
            #    port = 21
            #
            #    # Determines the uername used to login
            #    user = "username"
            #
            #    # Determines the password used to authenticate
            #    password = "secretPassword"
            #
            #    # Contains the base path to mount (Default is "/")
            #    basePath = "/data/somewhere"
            #
            #    # Specifies the maximal number of idle connections kept in the pool (Default is 1)
            #    # maxIdle = 1
            #
            #    # Specifies the maximal number of active connections (Default is 5)
            #    # maxActive = 5
            #
            #    # Specifies the connect timeout in milliseconds (Default is 10000)
            #    # connectTimeoutMillis = 10000
            #
            #    # Specifies the read / IO timeout in milliseconds (Default is 10000)
            #    # readTimeoutMillis = 10000
            #
            #    # Specifies the idle timeout in milliseconds. This is the maximal duration for with an unused
            #    # connection is kept open. (Default is 600_000 = 600s = 10 min)
            #    # idleTimeoutMillis = 600000
            #
            #    # Specifies the wait time in milliseconds. If the connection pool is exhaused, the is the maximal
            #    # duration we wait for a connection to become available. (Default is 10000)
            #    # connectTimeoutMillis = 10000
            #
            # }
        }

        downlink {
            # Provides the configuration of the built-in SSH (SCP/SFTP) server.
            ssh {
                # Specifies the port to listen on. Use 0 to disable the server or 22 to run it on the common SSH port.
                port = 0

                # Contains the path to the serialized host key. Note that an external file should be used which
                # is persisted across restarts and patches as otherwise the SSH connection looses its trust.
                hostKeyFile = "hostkey.ser"

                # Specifies the idle timeout for connections.
                idleTimeout = 10m

                # Specifies the read timeout for socket operations.
                readTimeout = 30s
            }

            # Defines the settings of the built-in FTP server.
            ftp {
                # Specifies the port to listen on. Use 0 to disable the server or 21 to run it on the common FTP port.
                port = 0

                # Specifies the port range to listen on for the data connection in passive mode, use any by default
                # Examples:
                # 2300               only use port 2300 as the passive port
                # 2300-2399          use all ports in the range
                # 2300-              use all ports larger than 2300
                # 2300, 2305, 2400-  use 2300 or 2305 or any port larger than 2400
                passivePorts = ""

                # Specifies the IP address to bind to. Leave empty to use all IP addresses.
                bindAddress = ""

                # Specifies the IP address clients have to connect to in passive mode.
                passiveExternalAddress = ""

                # Specifies the max. login failures before disconnecting.
                maxLoginFailures = 5

                # Specifies the max. number of concurrent clients.
                maxClients = 100

                # Specifies the max. number of threads to utilize.
                maxThreads = 10

                # Specifies the idle timeout for connections.
                idleTimeout = 10m

                # Specifies the max. connections per IP.
                maxConnectionsPerIp = 5

                # Specifies the JKS keystore to use for FTPS.
                keystore = ""

                # Specifies the keystore password.
                keystorePassword = ""

                # Specifies the key alias to use.
                keyAlias = ""

                # Determines if FTPS should be forced or not.
                forceSSL = false
            }
        }
    }

}



security {

    passwordMinLength = 4
    passwordSaneLength = 6

    # Specifies for how long generated passwords should be displayed.
    showGeneratedPasswordFor = 5 days

    scopes.default {
        manager = "tenants"
        system-tenant = "1"
        loginCookieTTL = 90 days
        available-languages = []
    }

    permissions {
        permission-manage-tenants       : "Required to manage tenants of the system"
        permission-manage-system-users  : "Required to manage user accounts of the system tenant"
        permission-manage-user-accounts : "Required to manage user accounts"
        permission-delete-user-accounts : "Required to delete user accounts"
        permission-manage-code-lists    : "Required to manage code lists"
        permission-system-protocols     : "Required to view protocols like logs, errors, mails, all audit logs"
        permission-system-cluster       : "Required to view and manage the cluster state"
        permission-audit-logs           : "Required to view audit logs for the own tenant"
        permission-system-journal       : "Required to view the system journal"
        permission-select-tenant        : "Required to switch to another tenant"
        permission-select-user-account  : "Required to switch to another user"
        permission-execute-jobs         : "Required to execute jobs"
        permission-manage-scheduler     : "Required to plan and edit scheduled jobs"

        # legacy storage framework...
        permission-manage-files         : "Required to manage well known buckets in the storage system"
        permission-manage-admin-files   : "Required to access administrative buckets in the storage system"
        # end of legacy...

        permission-manage-processes     : "Required to view processes of other users within the same tenant"
        permission-manage-all-processes : "Required to view processes of all users and tenants"
        permission-view-rate-limits     : "Required to view tenant-wide rate limits"
        permission-control-disaster-mode: "Required to enable or disable the maintenance / disaster mode"
        feature-user-account-config     : "Required (most probably as tenant permission) to provide custom configurations for user accounts"
    }

    roles = [ "system-administrator", "user-administrator", "administrator", "jobs-manager", "jobs-execution", "file-manager" ]

    tenantPermissions = [
    ]

    profiles {

        # If a user belongs to the system tenant, we set the member&affiliate flag
        flag-system-tenant {
            priority = 50
            flag-system-tenant-member = true
            flag-system-tenant-affiliate = true
        }

        # Grants the system administration and system user management permissions and flags to the role
        # "system-administrator"...
        system-administrator {
            priority = 60
            flag-system-administrator = true
            permission-manage-system-users = true
        }

        flag-system-administrator {
            priority = 110
            permission-manage-tenants = true
            permission-manage-code-lists = true
            permission-system-protocols = true
            permission-system-cluster = true
            permission-system-journal = true
            permission-system-console = true
            permission-system-timing = true
            permission-system-notify-state = true
            permission-system-load = true
            permission-manage-all-processes = true
            permission-view-rate-limits = true
            permission-delete-user-accounts = true
            permission-control-disaster-mode = true

            # By default we only permit sys admins to change the configs of any user in any
            # tenant as this is quite a specific and dangerous task...
            feature-user-account-config = true
        }

        user-administrator {
            priority = 120
            permission-manage-user-accounts = true
            permission-select-user-account = true
        }

        administrator {
            priority = 130
            permission-select-tenant = true
            permission-execute-jobs = true
            permission-manage-scheduler = true
            permission-view-scope-default-config = true
            permission-manage-files = true
            permission-manage-admin-files = true
            permission-system-audit-logs = true
            permission-manage-processes = true
            permission-view-rate-limits = true
        }

        jobs-manager {
            priority = 140
            permission-execute-jobs = true
            permission-manage-scheduler = true
            permission-manage-processes = true
        }

        jobs-execution {
            priority = 150
            permission-execute-jobs = true
        }

        # legacy storage framework...
        file-manager {
            priority = 160
            permission-manage-files = true
        }
    }

    # defines the packages and upgrades for the different scopes
    packages {

        #The 'tenant' scope used by the tenants framework (TenantUserManager etc.)
        tenant {
            # the packages of the scope
            packages = []

            # the upgrades of the scope
            upgrades = []
        }

        # indicates which permissions are needed to show a specific permission for a user
        # permissions which have no requirements can be omitted
        # have to be key-value pairs, in which the key is the permission in question and the value is the required permission
        required-permissions-for-permission {

        }
    }
}

# Specifies cache sizes used by the biz platform
cache {

    tenants-users {
        maxSize = 100
        ttl = 1 hour
    }

    tenants-roles {
        maxSize = 100
        ttl = 1 hour
    }

    tenants-children {
        maxSize = 256
        ttl = 1 hour
    }

    tenants-tenants {
        maxSize = 32
        ttl = 1 hour
    }

    tenants-configs {
        maxSize = 100
        ttl = 1 hour
    }

    storage-directories {
        maxSize = 8192
        ttl = 1 hour
    }

    storage-filenames {
        maxSize = 8192
        ttl = 1 hour
    }

    storage-physical-keys {
        maxSize = 16384
        ttl = 1 hour
    }

    storage-paths {
        maxSize = 8192
        ttl = 1 hour
    }

    # Legacy
    storage-object-metadata {
        maxSize = 16384
        ttl = 1 hour
    }

    # Legacy
    virtual-objects {
        maxSize = 16384
        ttl = 1 hour
    }

    processes-first-level {
        maxSize = 128
        ttl = 10 seconds
    }

    processes-second-level {
        maxSize = 256
        ttl = 10 minutes
    }

    standby-processes {
        maxSize = 1024
        ttl = 1 hour
    }

    codelists-values {
        maxSize = 4096
        ttl = 1 hour
    }

    objectstores-buckets {
        maxSize = 128
        ttl = 1 hour
    }

    metrics {
        maxSize = 8192
        ttl = 1 hour
    }
}

# Specifies thread pools used by the biz platform
async.executor {
    # This work queue is shared by all transfer managers across all object stores and
    # used for multipart up- and downloads.
    s3 {
        poolSize = 10
        queueLength = 0
    }

    # This executor collects all requests which are served via the storage framework. These requests might
    # be blocked as we might need to wait for a conversion to finish.
    storage-conversion-delivery {
        poolSize = 16
        queueLength = 128
    }

    # This executor is used by the storage framework (layer 2) to actually perform the conversion / generation of
    # a variant of a blob.
    storage-conversion {
        poolSize = 2
        queueLength = 1024
    }
}

# By default we use the smart lock manager. This detects the presence of redis and uses cluster-wide locks
# or otherwise uses fast local locks within the JVM. to enforce local locks, use "java".
# Another approach for clusters without Redis is using an SQL Database to implement locks distributed locks
# which is available via "sql" (SQLLockManager).
locks.manager = "smart"

# Determines how "Sequences" are stored and computed. By default a "smart" strategy is used which either
# checks if a "sql" database or a "mongo" database is ready and picks the right strategy. If both are
# available the effective startegy can be determined by setting an explicit value here.
sequences.strategy = "smart"

# Provides some aliases to simplify importing user accounts
importer.aliases {
    sqluseraccount {
        userAccountData_email: [ "e-mail", "EMail" ]
        userAccountData_externalLoginRequired: [ "externalLoginRequired" ]
        userAccountData_person_title: [ "title" ]
        userAccountData_person_salutation: [ "salutation" ]
        userAccountData_person_firstname: [ "firstname" ]
        userAccountData_person_lastname: [ "lastname" ]
        userAccountData_login_username: [ "username", "user" ]
        userAccountData_login_generatedPassword: [ "password", "$LoginData.password" ]
        userAccountData_login_accountLocked: [ "locked" ]
        userAccountData_permissions_permissions: [ "roles", "permissions" ]
    }

    mongouseraccount {
        userAccountData_email: [ "e-mail", "EMail" ]
        userAccountData_externalLoginRequired: [ "externalLoginRequired" ]
        userAccountData_person_title: [ "title" ]
        userAccountData_person_salutation: [ "salutation" ]
        userAccountData_person_firstname: [ "firstname" ]
        userAccountData_person_lastname: [ "lastname" ]
        userAccountData_login_username: [ "username", "user" ]
        userAccountData_login_generatedPassword: [ "password", "$LoginData.password" ]
        userAccountData_login_accountLocked: [ "locked", "$LoginData.accountLocked" ]
        userAccountData_permissions_permissions: [ "roles", "permissions", "$PermissionData.permissions" ]
    }

    sqlcodelistentry {
        codeListEntryData_code: [ "code" ]
        codeListEntryData_priority: [ "priority" ]
        codeListEntryData_value: [ "value" ]
        codeListEntryData_additionalValue: [ "additionalValue" ]
        codeListEntryData_description: [ "description" ]
    }

    mongocodelistentry {
        codeListEntryData_code: [ "code" ]
        codeListEntryData_priority: [ "priority" ]
        codeListEntryData_value: [ "value" ]
        codeListEntryData_additionalValue: [ "additionalValue" ]
        codeListEntryData_description: [ "description" ]
    }
}

# Determines which databases can be directly queried via /system/sql
# By default we allow to access the system database and clickhouse, which is the statistics database.
# Note that if a database is listed here, but not present, it will be ignored.
jdbc.selectableDatabases = [ system, clickhouse ]

mixing.legacy.SQLTenant.rename {
    "tenantData_configString": "tenantData_permissions_configString"
    "tenantData_packageData_additionalPermissions": "tenantData_permissions_permissionString"
}

mixing.legacy.SQLUserAccount.rename {
    "userAccountData_permissions_permissions": "userAccountData_permissions_permissionString"
}

# Contains settings which are used by the analytics and metrics sub-system.
analytics {
    # Contains settings regarding the metric computation for UserAccounts...^
    user-accounts {
        # Contains the number of days for which the user-activity metric is computed. This metric is the
        # percentage of days of the given period in which the user was seen.
        observationPeriodDays = 45

        # Contains the minimal number of days on which the user has to been in the above defined period
        # so that the "active-user" flag is toggled.
        minDaysForActiveUsers = 1

        # Contains the minimal number of days on which the user has to been in the above defined period
        # so that the "frequent-user" flag is toggled.
        minDaysForFrequentUsers = 4
    }
}
